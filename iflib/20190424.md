# Attendees

- Olivier Cochard
- Andrew Gallatin
- Stephen Hurd
- Patrick Kelsey
- Marius Strobl
- John Baldwin

# Notes

## LRO

The current / old LRO scheme only works with a low number of
connections since a large number of connections overflows the table of
"in progress" connections and the overflow packets are all passed up
without coalescing.  The LRO from Hans instead builds an array of
packets sorted by RSS hash and arrival time.  At end-of-interrupt,
this sorted array is fed to the LRO engine, thus greatly increasing
the odds of coalescing packets from the same connection when there are
packets from many connections intermixed in the queue.  This uses the
`lro_queue_mbuf()` interface.  In order for this to be effective,
there must be a large (O(1000)) batch of packets to process.  There
are 2 tasks for converting iflib to this:

1. Induce iflib to create a large batch of packets
2. switch to the new `lro_queue_mbuf()` API

Both **Stephen** and **Drew** are interested in working on this.

## Changes in Review

- https://reviews.freebsd.org/D19949 - Needs review for any ABI questions

  **Marius** says it looks good to him.  **Stephen** says that is sufficient.

- https://reviews.freebsd.org/D19946

  **Marius** says it has some style fixes, but generally looks good.

## `if_alloc_dev`

**Drew** asks if anyone objects to using `if_alloc_dev` for initial
NUMA awareness.  No one objects.

## PPS

**Drew** found that on ixl, netmap vs plain transmit was 32M pps vs
12M pps.  For mce (mlx5), Drew saw 19M for netperf UDP.

**Marius** notes that iflib doesn't use `mp_ring` for netmap.

**Drew** thinks `mp_ring` is the best candidate for the performance
difference.

**John** thinks `mp_ring` is a replacement for `buf_ring` which was
designed as Drew notes for machines where number of CPUs >> number of
queues.  However, modern 10G+ NICs have many more queues that are the
same as number of CPUs.

**Drew** confirms that cxgbe has the same bottleneck (12M pps), but
netmap sends at 45M pps.

**Stephen** finds the size of `mp_ring` (2k entries) odd since it is
smaller than a hardware TX ring.

**John** notes that on Chelsio (where `mp_ring` originated), the
default TX ring is 1k, so that is where that number probably came
from.

## More about LRO

**Marius** notes two things about iflib's rxeof function:

1. Strange hack to determine if LRO is possible via a helper function
   that duplicates some LRO logic.  Marius wants to know if it will go
   away when using the new API.  Drew and Stephen say it should go
   away in the new API.
   
2. Currently iflib shovels up any packets to LRO stack, even if it's
   not TCP.  Marius thinks it is simple to use hardware offload bits
   to determine if it's TCP at all.

   **Drew** isn't sure iflib itself has that info from the driver.

   **John** wonders if we can have a flag to tell iflib it can trust
   `CSUM_TCP`.
   
   **Stephen** thinks SCTP can be LRO'd as well?

## Netmap Issues

**Drew** says if you push netmap too hard on ixl it drops to passing
no traffic at all.

**John** suggests it sounds like there is a kind of missing wakeup
issue (e.g. when the queue is full?).

**Drew** is not going to fix it.

**Stephen** says that before his fixes last year
([r333686](https://svnweb.freebsd.org/base?view=revision&revision=333686),
[r334862](https://svnweb.freebsd.org/base?view=revision&revision=334862),
and
[r336559](https://svnweb.freebsd.org/base?view=revision&revision=336559)),
netmap would always timeout regardless of load.

Various discussion.  Patrick notes that iflib in general does not use
tx interrupts but processes tx completions on demand (lazily).  This
seems at odds with netmap's design.

**John** suggests using sbintime_t with a fixed timeout (e.g. 1 ms)
instead of `hz` for the callout frequency.

Consensus is we need someone who cares about netmap to look at it, but
no one here cares currently.

## CPU Cores vs Interrupts

**Stephen** asks Olivier if he normally has more CPU cores than queues
in his tests.  Stephen has a patch to distribute interrupts among CPUs
when you have fewer queues than cores and is curious if that will help
Olivier's forwarding benchmarks.

**Ollivier** posts a
[link](https://bsdrp.net/documentation/technical_docs/bench_lab) to
his test setup.
